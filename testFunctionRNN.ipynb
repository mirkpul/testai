{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start creating a dummy dataset with Function data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:\n",
      "(100000, 3)\n",
      "(100000, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "funData = []\n",
    "resultData = []\n",
    "\n",
    "for x in range(1,100001):\n",
    "    funData.append([x,x+1,x+2])\n",
    "    resultData.append([(x+x+1+x+2)*2])\n",
    "\n",
    "print(\"Dataset size:\")\n",
    "print(np.array(funData).shape)\n",
    "print(np.array(resultData).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.scaling data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_features = StandardScaler()\n",
    "X_scaled = scaler_features.fit_transform(funData)\n",
    "scaler_labels = StandardScaler()\n",
    "Y_scaled = scaler_labels.fit_transform(resultData)\n",
    "#X_scaled = np.array(funData)\n",
    "#Y_scaled = np.array(resultData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. splitting into train and test\n",
    "train_size_perc = 0.9\n",
    "train_limit = len(X_scaled) * train_size_perc\n",
    "X_scaled_train = X_scaled[:int(train_limit)]\n",
    "X_scaled_test = X_scaled[int(train_limit):]\n",
    "Y_scaled_train = Y_scaled[:int(train_limit)]\n",
    "Y_scaled_test = Y_scaled[int(train_limit):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89996, 5, 3)\n",
      "(9996, 5, 3)\n",
      "(89996, 1)\n",
      "(9996, 1)\n"
     ]
    }
   ],
   "source": [
    "window = 5\n",
    "features_reshaped_train = []\n",
    "features_reshaped_test = []\n",
    "labels_reshaped_train = []\n",
    "labels_reshaped_test = []\n",
    "\n",
    "#resizing X_scaled_train sliding window from the beginning to the end\n",
    "for i in range(0, len(X_scaled_train) - window +1):\n",
    "    #appending sub list of window size\n",
    "    features_reshaped_train.append(X_scaled_train[i:i + window][:,[0,1,2]])\n",
    "    #feeding labels with third features of the last window item. Optional\n",
    "    #labels.append(Y_scaled[i + window - 1])\n",
    "\n",
    "#resizing X_scaled_test sliding window from the beginning to the end\n",
    "for i in range(0, len(X_scaled_test) - window +1):\n",
    "    #appending sub list of window size\n",
    "    features_reshaped_test.append(X_scaled_test[i:i + window][:,[0,1,2]])\n",
    "    #feeding labels with third features of the last window item. Optional\n",
    "    #labels.append(Y_scaled[i + window - 1])\n",
    "\n",
    "#resizing Y_scaled_train sliding window from the beginning to the end\n",
    "for i in range(0, len(Y_scaled_train) - window + 1):\n",
    "    #appending sub list of window size\n",
    "    labels_reshaped_train.append(Y_scaled_train[i + window - 1])\n",
    "    #feeding labels with third features of the last window item. Optional\n",
    "    #labels.append(Y_scaled[i + window - 1])\n",
    "\n",
    "#resizing Y_scaled_test sliding window from the beginning to the end\n",
    "for i in range(0, len(Y_scaled_test) - window +1):\n",
    "    #appending sub list of window size\n",
    "    labels_reshaped_test.append(Y_scaled_test[i + window - 1])\n",
    "    #feeding labels with third features of the last window item. Optional\n",
    "    #labels.append(Y_scaled[i + window - 1])\n",
    "    \n",
    "print(np.array(features_reshaped_train).shape)\n",
    "print(np.array(features_reshaped_test).shape)\n",
    "print(np.array(labels_reshaped_train).shape)\n",
    "print(np.array(labels_reshaped_test).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create an ANN predicting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "def create_model(window):\n",
    "    model = Sequential()\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=True, input_shape=(window,3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(50, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense (1))\n",
    "    model.compile(loss='mse',optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 89996 samples, validate on 9996 samples\n",
      "Epoch 1/3\n",
      "89996/89996 [==============================] - 31s 345us/step - loss: 0.0189 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "89996/89996 [==============================] - 28s 310us/step - loss: 0.0015 - acc: 0.0000e+00 - val_loss: 0.0109 - val_acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "89996/89996 [==============================] - 29s 318us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0084 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7885ea0e80>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model(window)\n",
    "\n",
    "X_train = np.array(features_reshaped_train)\n",
    "Y_train = np.array(labels_reshaped_train)\n",
    "X_test = np.array(features_reshaped_test)\n",
    "Y_test = np.array(labels_reshaped_test)\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size=128,epochs=3,verbose=1, validation_data = (X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 38378.546875]]\n"
     ]
    }
   ],
   "source": [
    "test_data = [[6000,6001,6002],[6001,6002,6003],[6002,6003,6004],[6003,6004,6005],[6004,6005,6006]]\n",
    "\n",
    "test_data = scaler_features.transform(test_data)\n",
    "\n",
    "test_predict = scaler_labels.inverse_transform(model.predict(test_data.reshape(1,5,3)))\n",
    "print(test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
